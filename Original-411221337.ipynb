
Original 411221337.ipynb
Original 411221337.ipynbC_
Files
..
Drop files to upload them to session storage.
Disk
73.06 GB available
Student Name: Ayanda Sibusiso Dlamini

Student ID : 411221337

Department : CSIE

Grade: Juniour

ASSIGNMENT 1
CNN model using the PyTorch framework with the Cat and Dog

The process of building a Convolutional Neural Network always involves four major steps.

Step - 1 : Convolution

Step - 2 : Pooling

Step - 3 : Flattening

Step - 4 : Full connection


[4]
9s
# Check PyTorch & GPU
import torch, torchvision
print("torch:", torch.__version__, "cuda:", torch.cuda.is_available())
!nvidia-smi -L || true   # shows GPU if available (safe if not)

torch: 2.8.0+cu126 cuda: True
GPU 0: Tesla T4 (UUID: GPU-515288ad-340f-99fc-2fb1-edbfa80a3a65)
ðŸ§© Step 1: Mount Google Drive in Colab


[5]
19s
from google.colab import drive
drive.mount('/content/drive')

Mounted at /content/drive
Locating My Dataset


[6]
0s
!ls /content/drive/MyDrive/GenAI/cats_dogs

test_set.zip
Unzip the Dataset


[7]
3s
import zipfile
import os

zip_path = '/content/drive/MyDrive/GenAI/cats_dogs/test_set.zip'
extract_path = '/content/dataset'

# Extract
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print("Extracted files to:", extract_path)

Extracted files to: /content/dataset

[ ]
!ls /content/dataset

test_set
Split test_set into train and val


[8]
0s
import os
import glob
import shutil
import random

random.seed(42)

src_dir = '/content/dataset/test_set'
output_root = '/content/dataset'
val_split = 0.2  # 20% for validation

for cls in ['cats', 'dogs']:
    imgs = glob.glob(os.path.join(src_dir, cls, '*.*'))
    random.shuffle(imgs)
    split_idx = int(len(imgs) * (1 - val_split))

    # train folder
    train_folder = os.path.join(output_root, 'train', cls)
    os.makedirs(train_folder, exist_ok=True)
    for img_path in imgs[:split_idx]:
        shutil.copy(img_path, train_folder)

    # val folder
    val_folder = os.path.join(output_root, 'val', cls)
    os.makedirs(val_folder, exist_ok=True)
    for img_path in imgs[split_idx:]:
        shutil.copy(img_path, val_folder)

print("Dataset split complete!")
print("Train folder:", os.listdir(os.path.join(output_root, 'train')))
print("Val folder:", os.listdir(os.path.join(output_root, 'val')))

Dataset split complete!
Train folder: ['cats', 'dogs']
Val folder: ['cats', 'dogs']
Load the datasets with DataLoaders


[9]
0s
# Import datasets and transforms from torchvision for handling image data and augmentations
from torchvision import datasets, transforms
# Import DataLoader to efficiently load batches of data during training
from torch.utils.data import DataLoader

# Define the target size for images (width and height)
img_size = 128
# Define the batch size for training and validation
batch_size = 32

# Define transformations for training data
train_tfms = transforms.Compose([
    # Resize all images to img_size x img_size
    transforms.Resize((img_size,img_size)),
    # Apply random horizontal flip for data augmentation
    transforms.RandomHorizontalFlip(),
    # Convert images to PyTorch tensors
    transforms.ToTensor()
])

# Define transformations for validation data
val_tfms = transforms.Compose([
    # Resize images to img_size x img_size
    transforms.Resize((img_size,img_size)),
    # Convert images to PyTorch tensors
    transforms.ToTensor()
])

# Load training dataset from folder and apply training transformations
train_dataset = datasets.ImageFolder('/content/dataset/train', transform=train_tfms)
# Load validation dataset from folder and apply validation transformations
val_dataset = datasets.ImageFolder('/content/dataset/val', transform=val_tfms)

# Create DataLoader for training dataset with batch processing and shuffling
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
# Create DataLoader for validation dataset with batch processing, no shuffling
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# Print the class labels detected in the dataset
print("Classes:", train_dataset.classes)
# Print the total number of training samples
print("Number of training samples:", len(train_dataset))
# Print the total number of validation samples
print("Number of validation samples:", len(val_dataset))

Classes: ['cats', 'dogs']
Number of training samples: 1617
Number of validation samples: 406
Definition the CNN model and training


[10]
27s
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# Check if GPU is available and use it, otherwise fallback to CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define a simple Convolutional Neural Network for binary classification (cats vs dogs)
class SimpleCNN(nn.Module):

Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:05<00:00,  8.64it/s]
Epoch [1/5], Loss: 0.7781
Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:05<00:00,  8.78it/s]
Epoch [2/5], Loss: 0.6674
Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:04<00:00, 10.58it/s]
Epoch [3/5], Loss: 0.6511
Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:05<00:00,  9.62it/s]
Epoch [4/5], Loss: 0.6201
Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:05<00:00,  9.67it/s]Epoch [5/5], Loss: 0.5758
Model saved successfully!

Set up a validation loop and also plot graphs for training and validation performance (loss and accuracy).

Gemini

[11]
3m
import matplotlib.pyplot as plt

# Set the number of epochs to match the original notebook for better accuracy
epochs = 30

# Lists to store training and validation metrics
train_losses = []
train_accuracies = []
val_losses = []
val_accuracies = []

import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# Check if GPU is available and use it, otherwise fallback to CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define a simple Convolutional Neural Network for binary classification (cats vs dogs)


Now i need to improve the accuracy by incorporating Batch Normalization and Dropout.

Colab paid products - Cancel contracts here
